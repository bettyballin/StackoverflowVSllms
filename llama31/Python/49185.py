import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load data\nsummaries = [...]  # list of summaries\n\n# Preprocessing\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n    return ' '.join(tokens)\n\n# Vectorization\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform([preprocess_text(summary) for summary in summaries])\n\n# Similarity measurement\ndef get_similar_summaries(query_summary, num_results=5):\n    query_vector = vectorizer.transform([preprocess_text(query_summary)])\n    similarities = cosine_similarity(query_vector, vectors).flatten()\n    top_indices = np.argsort(-similarities)[:num_results]\n    return [summaries[i] for i in top_indices]\n\n# Example usage\nquery_summary = "Building a house in New York"\nsimilar_summaries = get_similar_summaries(query_summary)\nprint(similar_summaries)