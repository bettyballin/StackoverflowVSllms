import re\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef word_frequency(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation and special characters\n    text = re.sub(r'[^\w\s]', '', text)\n\n    # Tokenize text into words\n    words = nltk.word_tokenize(text)\n\n    # Remove stopwords (articles, pronouns, etc.)\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Lemmatize words to handle stemming and plurals\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    # Count word frequency\n    frequency = Counter(words)\n\n    return frequency\n\n# Example usage:\ntext = "This is an example sentence. This sentence is just an example."\nfrequency = word_frequency(text)\nprint(frequency)