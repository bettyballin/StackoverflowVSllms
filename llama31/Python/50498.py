import nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\n\ndef summarize_text(text, threshold=0.5):\n    # Tokenize sentences\n    sentences = sent_tokenize(text)\n\n    # Count tags and nouns in each sentence\n    tag_count = {}\n    noun_count = {}\n    for sentence in sentences:\n        tokens = nltk.word_tokenize(sentence)\n        tags = nltk.pos_tag(tokens)\n        nouns = [token for token, tag in tags if tag.startswith('NN')]\n        tag_count[sentence] = len(tags)\n        noun_count[sentence] = len(nouns)\n\n    # Calculate importance score for each sentence\n    scores = {}\n    for sentence in sentences:\n        score = (tag_count[sentence] + noun_count[sentence]) / (len(sentences) * 2)\n        scores[sentence] = score\n\n    # Select top-scoring sentences\n    top_sentences = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    summary = ' '.join([sentence for sentence, score in top_sentences if score >= threshold])\n\n    return summary\n\ntext = '...'  # input text\nprint(summarize_text(text))