import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\n\ndef check_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = soup.find_all('a')\n\n    for link in links:\n        href = link.get('href')\n        if href:\n            absolute_url = urljoin(url, href)\n            parsed_url = urlparse(absolute_url)\n            if parsed_url.netloc == urlparse(url).netloc:\n                try:\n                    response = requests.head(absolute_url)\n                    if response.status_code >= 400:\n                        print(f"Broken link: {absolute_url}")\n                except requests.RequestException:\n                    print(f"Error checking link: {absolute_url}")\n\ncheck_links('https://example.com')