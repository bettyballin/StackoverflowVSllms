import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef relatedness_score(new_question, existing_question):\n    # Preprocessing\n    new_tokens = word_tokenize(new_question)\n    existing_tokens = word_tokenize(existing_question)\n    stop_words = set(stopwords.words('english'))\n    stemmer = PorterStemmer()\n\n    new_tokens = [stemmer.stem(token) for token in new_tokens if token not in stop_words]\n    existing_tokens = [stemmer.stem(token) for token in existing_tokens if token not in stop_words]\n\n    # Similarity measurement\n    intersection = set(new_tokens) & set(existing_tokens)\n    union = set(new_tokens) | set(existing_tokens)\n    jaccard_similarity = len(intersection) / len(union)\n\n    return jaccard_similarity\n\n# Example usage\nnew_question = "How to implement a related degree measure algorithm?"\nexisting_questions = [\n    "How to implement a related degree measure algorithm in Python?",\n    "What is the best way to measure the relatedness of two questions?",\n    "How to use NLTK for text processing?"\n]\n\nscores = []\nfor existing_question in existing_questions:\n    score = relatedness_score(new_question, existing_question)\n    scores.append((existing_question, score))\n\nscores.sort(key=lambda x: x[1], reverse=True)\nprint(scores)