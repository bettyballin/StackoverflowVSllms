import torch\nfrom transformers import BertTokenizer, BertModel\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Define phrases\nphrase1 = 'This is a test phrase'\nphrase2 = 'This is another test phrase'\n\n# Tokenize phrases\ninputs1 = tokenizer.encode_plus(phrase1, \n                                add_special_tokens=True, \n                                max_length=512, \n                                return_attention_mask=True, \n                                return_tensors='pt')\n\ninputs2 = tokenizer.encode_plus(phrase2, \n                                add_special_tokens=True, \n                                max_length=512, \n                                return_attention_mask=True, \n                                return_tensors='pt')\n\n# Get sentence embeddings\noutputs1 = model(inputs1['input_ids'], attention_mask=inputs1['attention_mask'])\noutputs2 = model(inputs2['input_ids'], attention_mask=inputs2['attention_mask'])\n\n# Calculate cosine similarity\nsimilarity = torch.cosine_similarity(outputs1.last_hidden_state[:, 0, :], outputs2.last_hidden_state[:, 0, :])\n\nprint(similarity.item())