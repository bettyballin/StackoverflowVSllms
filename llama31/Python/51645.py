import numpy as np\n\n# Define the word lists and their corresponding weights\nword_lists = {\n    'BNC': 0.4,\n    'WordNet': 0.3,\n    'Internal Corpus': 0.3\n}\n\n# Define the frequency data for each word\nword_frequencies = {\n    'word1': {'BNC': 10, 'WordNet': 5, 'Internal Corpus': 20},\n    'word2': {'BNC': 50, 'WordNet': 10, 'Internal Corpus': 5},\n    # ...\n}\n\n# Calculate the weighted scores for each word\nweighted_scores = {}\nfor word, frequencies in word_frequencies.items():\n    score = 0\n    for list_name, frequency in frequencies.items():\n        score += frequency * word_lists[list_name]\n    weighted_scores[word] = score\n\n# Normalize the scores to a common range (0 to 1)\nmax_score = max(weighted_scores.values())\nmin_score = min(weighted_scores.values())\nnormalized_scores = {word: (score - min_score) / (max_score - min_score) for word, score in weighted_scores.items()}\n\n# Classify words as common or rare based on a threshold value\nthreshold = 0.5\ncommon_words = {word for word, score in normalized_scores.items() if score >= threshold}\nrare_words = {word for word, score in normalized_scores.items() if score < threshold}