import nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the dataset\narticles = [...]  # list of article texts\n\n# Preprocessing and vectorization\nvectorizer = TfidfVectorizer(stop_words='english')\ntfidf_matrix = vectorizer.fit_transform(articles)\n\n# Similarity measurement\ndef find_similar_articles(query, num_results=5):\n    query_vector = vectorizer.transform([query])\n    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n    top_indices = np.argsort(-similarities)[:num_results]\n    return [articles[i] for i in top_indices]\n\n# Example usage\nquery = "This is a sample query"\nsimilar_articles = find_similar_articles(query)\nprint(similar_articles)