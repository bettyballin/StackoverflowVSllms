#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <curl/curl.h>\n#include <libxml/parser.h>\n#include <libxml/tree.h>\n\n// Structure to hold webpage content\nstruct MemoryStruct {\n    char *memory;\n    size_t size;\n};\n\n// Callback for curl to write data\nstatic size_t WriteMemoryCallback(void *contents, size_t size, size_t nmemb, void *userp) {\n    size_t realsize = size * nmemb;\n    struct MemoryStruct *mem = (struct MemoryStruct *)userp;\n\n    char *ptr = realloc(mem->memory, mem->size + realsize + 1);\n    if (ptr == NULL) {\n        printf("Not enough memory\n");\n        return 0;\n    }\n\n    mem->memory = ptr;\n    memcpy(&(mem->memory[mem->size]), contents, realsize);\n    mem->size += realsize;\n    mem->memory[mem->size] = 0;\n\n    return realsize;\n}\n\n// Function to initialize CURL and fetch the page content\nchar *fetchPageContent(const char *url) {\n    CURL *curl_handle;\n    CURLcode res;\n\n    struct MemoryStruct chunk;\n    chunk.memory = malloc(1);\n    chunk.size = 0;\n\n    curl_global_init(CURL_GLOBAL_ALL);\n    curl_handle = curl_easy_init();\n    curl_easy_setopt(curl_handle, CURLOPT_URL, url);\n    curl_easy_setopt(curl_handle, CURLOPT_WRITEFUNCTION, WriteMemoryCallback);\n    curl_easy_setopt(curl_handle, CURLOPT_WRITEDATA, (void *)&chunk);\n\n    res = curl_easy_perform(curl_handle);\n    if (res != CURLE_OK) {\n        fprintf(stderr, "curl_easy_perform() failed: %s\n", curl_easy_strerror(res));\n        free(chunk.memory);\n        return NULL;\n    }\n\n    curl_easy_cleanup(curl_handle);\n    return chunk.memory;\n}\n\n// Function to create XML sitemap\nvoid createSitemap(const char *filename, char **urls, int url_count) {\n    xmlDocPtr doc = NULL;\n    xmlNodePtr root_node = NULL;\n    xmlNodePtr url_node = NULL;\n\n    doc = xmlNewDoc(BAD_CAST "1.0");\n    root_node = xmlNewNode(NULL, BAD_CAST "urlset");\n    xmlNsPtr ns = xmlNewNs(root_node, BAD_CAST "http://www.sitemaps.org/schemas/sitemap/0.9", NULL);\n    xmlSetNs(root_node, ns);\n    xmlDocSetRootElement(doc, root_node);\n\n    for (int i = 0; i < url_count; i++) {\n        url_node = xmlNewChild(root_node, NULL, BAD_CAST "url", NULL);\n        xmlNewChild(url_node, NULL, BAD_CAST "loc", BAD_CAST urls[i]);\n    }\n\n    xmlSaveFormatFileEnc(filename, doc, "UTF-8", 1);\n    xmlFreeDoc(doc);\n    xmlCleanupParser();\n}\n\nint main(int argc, char *argv[]) {\n    if (argc != 2) {\n        fprintf(stderr, "Usage: %s <URL>\n", argv[0]);\n        return 1;\n    }\n\n    const char *url = argv[1];\n    char *content = fetchPageContent(url);\n\n    if (content == NULL) {\n        fprintf(stderr, "Failed to fetch content from %s\n", url);\n        return 1;\n    }\n\n    // Here you would parse the content to extract URLs and avoid duplicates.\n    // For simplicity, we will use a static list of URLs.\n\n    char *urls[] = {\n        "http://example.com/",\n        "http://example.com/about/",\n        "http://example.com/contact/"\n    };\n    int url_count = sizeof(urls) / sizeof(urls[0]);\n\n    createSitemap("sitemap.xml", urls, url_count);\n\n    free(content);\n    return 0;\n}