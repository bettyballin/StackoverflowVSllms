import os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom concurrent.futures import ThreadPoolExecutor\n\n# List of URLs to download\nurls = [\n    'http://example.com/page1',\n    'http://example.com/page2',\n    # Add more URLs as needed\n]\n\ndef download_and_extract_text(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    text = soup.get_text()\n\n    # Generate a safe filename from the URL\n    filename = url.replace('http://', '').replace('https://', '').replace('/', '_') + '.txt'\n    \n    # Save the extracted text to a file\n    with open(filename, 'w') as file:\n        file.write(text)\n\n# Use threading for concurrent downloads\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    executor.map(download_and_extract_text, urls)\n\nprint("Download and text extraction complete.")