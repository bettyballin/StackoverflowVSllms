import collections\nimport math\n\ndef calculate_ngram_entropy(text, n=2):\n    # Generate n-grams\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    \n    # Calculate frequency distribution\n    frequency_distribution = collections.Counter(ngrams)\n    \n    # Calculate probabilities\n    total_ngrams = sum(frequency_distribution.values())\n    probabilities = [count / total_ngrams for count in frequency_distribution.values()]\n\n    # Calculate entropy\n    entropy = -sum(p * math.log2(p) for p in probabilities)\n    return entropy\n\n# Example usage\ntext = "your input text here"\nbigram_entropy = calculate_ngram_entropy(text, n=2)\ntrigram_entropy = calculate_ngram_entropy(text, n=3)\n\nprint(f"Bigram Entropy: {bigram_entropy}")\nprint(f"Trigram Entropy: {trigram_entropy}")