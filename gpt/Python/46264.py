import requests\n     from bs4 import BeautifulSoup\n     from urllib.parse import urljoin\n\n     def find_broken_links(url):\n         response = requests.get(url)\n         soup = BeautifulSoup(response.text, 'html.parser')\n         broken_links = []\n         for link in soup.find_all('a', href=True):\n             link_url = urljoin(url, link['href'])\n             try:\n                 link_response = requests.head(link_url, allow_redirects=True)\n                 if link_response.status_code == 404:\n                     broken_links.append(link_url)\n             except requests.RequestException:\n                 broken_links.append(link_url)\n         return broken_links\n\n     website_url = 'http://example.com'\n     broken_links = find_broken_links(website_url)\n     if broken_links:\n         print(f'Broken links found: {broken_links}')\n     else:\n         print('No broken links found.')