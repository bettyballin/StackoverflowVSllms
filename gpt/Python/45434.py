import collections\nimport re\n\ndef process_chunk(chunk, word_counter):\n    words = re.findall(r'\b\w+\b', chunk.lower())\n    for word in words:\n        word_counter[word] += 1\n\ndef find_high_frequency_words(file_path, chunk_size=1024):\n    word_counter = collections.Counter()\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        while True:\n            chunk = file.read(chunk_size)\n            if not chunk:\n                break\n            process_chunk(chunk, word_counter)\n            # Move back a few characters to ensure words are not broken\n            file.seek(file.tell() - 10)\n    \n    # Find the highest frequency words\n    max_freq = max(word_counter.values())\n    high_freq_words = [word for word, freq in word_counter.items() if freq == max_freq]\n    \n    return high_freq_words, max_freq\n\n# Example usage\nfile_path = 'path_to_your_text_book.txt'\nhigh_freq_words, max_freq = find_high_frequency_words(file_path)\nprint(f"High frequency words: {high_freq_words} with frequency: {max_freq}")