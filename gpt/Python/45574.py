from transformers import BertTokenizer, BertModel\nimport torch\nfrom scipy.spatial.distance import cosine\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embedding(phrase):\n    inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True)\n    outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n\ndef calculate_similarity(phrase1, phrase2):\n    embedding1 = get_embedding(phrase1)\n    embedding2 = get_embedding(phrase2)\n    similarity = 1 - cosine(embedding1, embedding2)\n    return similarity\n\n# Example usage\nphrase1 = "How are you?"\nphrase2 = "How do you do?"\nsimilarity = calculate_similarity(phrase1, phrase2)\nprint(f"Semantic Similarity: {similarity}")