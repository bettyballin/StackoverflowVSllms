import os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\n# URL of the directory listing\nbase_url = 'http://www.arthika.net/1234TB/new/Kuruvi/'\n\n# Create a directory to save the files\ndownload_dir = 'downloaded_files'\nos.makedirs(download_dir, exist_ok=True)\n\n# Make a request to the directory listing\nresponse = requests.get(base_url)\nresponse.raise_for_status()  # Check if the request was successful\n\n# Parse the HTML content\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find all the links\nlinks = soup.find_all('a')\n\n# Download each file\nfor link in links:\n    href = link.get('href')\n    if href and not href.startswith('?') and not href.startswith('/'):\n        file_url = urljoin(base_url, href)\n        file_name = os.path.join(download_dir, os.path.basename(href))\n        \n        print(f'Downloading {file_url}')\n        \n        file_response = requests.get(file_url)\n        file_response.raise_for_status()  # Check if the request was successful\n        \n        with open(file_name, 'wb') as file:\n            file.write(file_response.content)\n\nprint('All files downloaded successfully.')