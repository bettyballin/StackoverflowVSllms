from bs4 import BeautifulSoup\nimport os\n\ndef find_links_in_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    \n    soup = BeautifulSoup(content, 'html.parser')\n    links = []\n    \n    for a_tag in soup.find_all('a', href=True):\n        link = a_tag['href']\n        if not (link.startswith('#') or link.startswith('/') or '://' in link):  # Assuming internal relative paths without extensions\n            link += '.asp'\n        links.append(link)\n        a_tag['href'] = link\n    \n    with open(file_path[:-5] + '.asp', 'w', encoding='utf-8') as file:\n        file.write(str(soup))\n    \n    return links\n\ndef process_all_html_files(directory):\n    all_links = []\n    for root, _, files in os.walk(directory):\n        for file_name in files:\n            if file_name.endswith('.html'):\n                file_path = os.path.join(root, file_name)\n                print(f"Processing {file_path}")\n                links = find_links_in_file(file_path)\n                all_links.extend(links)\n    \n    return all_links\n\n# Usage\ndirectory_of_html_files = '/path/to/your/html/files'\nall_internal_links = process_all_html_files(directory_of_html_files)\nprint("All internal links found and updated:", all_internal_links)