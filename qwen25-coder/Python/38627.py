import pandas as pd\n\n# Read in chunks if the file is too large to fit into memory\nchunk_size = 10**6  # read 1 million rows at a time\nchunks = []\n\nfor chunk in pd.read_csv('largefile.csv', chunksize=chunk_size):\n    # Process each chunk (e.g., filter, transform)\n    chunks.append(chunk)\n\n# Combine all processed chunks into a single DataFrame\ndf = pd.concat(chunks)\n\n# Now you can operate on df as needed\nprint(df.head())