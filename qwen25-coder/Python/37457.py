import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef check_links_on_page(url):\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        all_links = set([urljoin(url, link.get('href')) for link in soup.find_all('a', href=True)])\n        \n        broken_links = []\n        for link in all_links:\n            try:\n                if not (requests.head(link).status_code < 400):\n                    broken_links.append(link)\n            except requests.exceptions.RequestException as e:\n                broken_links.append(link)\n        return broken_links\n    except Exception as e:\n        print(f"Error accessing {url}: {e}")\n        return []\n\nif __name__ == "__main__":\n    url = 'http://yourwebsite.com'\n    broken_links = check_links_on_page(url)\n    if broken_links:\n        print("Broken links found:")\n        for link in broken_links:\n            print(link)\n    else:\n        print("No broken links found.")