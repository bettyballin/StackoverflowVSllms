import os\nimport re\nfrom collections import defaultdict\n\ndef tokenize(text):\n    # Use regex to find words, convert to lower case\n    return [word.lower() for word in re.findall(r'\b\w+\b', text)]\n\ndef index_files(directory):\n    index = defaultdict(list)\n    stop_words = set(['the', 'and', 'is', 'etc'])  # Simple stop words\n\n    # Traverse directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    tokens = tokenize(content)\n                    # Index the file by its tokens\n                    for token in set(tokens) - stop_words:  # Remove stop words\n                        index[token].append(file_path)\n            except Exception as e:\n                print(f"Error reading {file_path}: {e}")\n    \n    return index\n\n# Usage\ndirectory_to_index = '/path/to/your/directory'\nindex = index_files(directory_to_index)\nprint(index)  # Print the resulting index for inspectio