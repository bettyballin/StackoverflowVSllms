import requests\n   from bs4 import BeautifulSoup\n   from concurrent.futures import ThreadPoolExecutor\n\n   def download_and_extract(url):\n       try:\n           response = requests.get(url)\n           soup = BeautifulSoup(response.text, 'html.parser')\n           text_content = soup.get_text(separator='\n', strip=True)\n           file_name = f"{url.replace('https://', '').replace('/', '_')}.txt"\n           with open(file_name, 'w', encoding='utf-8') as f:\n               f.write(text_content)\n       except Exception as e:\n           print(f"Failed to download or parse {url}: {e}")\n\n   urls = ['http://example.com/page1', 'http://example.com/page2']  # replace with your URLs\n\n   with ThreadPoolExecutor(max_workers=5) as executor:\n       executor.map(download_and_extract, urls)