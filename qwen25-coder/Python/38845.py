import os\nimport hashlib\nimport json\n\ndef hash_file(filepath):\n    """ Calculate SHA-256 hash of the given file """\n    sha_signature = hashlib.sha256()\n    with open(filepath, "rb") as f:\n        for byte_block in iter(lambda: f.read(4096), b""):\n            sha_signature.update(byte_block)\n    return sha_signature.hexdigest()\n\ndef create_backup(source_dir, backup_dir, metadata_path):\n    """ Create an incremental backup """\n    if not os.path.exists(metadata_path):\n        previous_hashes = {}\n    else:\n        with open(metadata_path) as f:\n            previous_hashes = json.load(f)\n\n    current_hashes = {}\n\n    for dirpath, subdirnames, filenames in os.walk(source_dir):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            relative_path = os.path.relpath(filepath, source_dir)\n            file_hash = hash_file(filepath)\n            current_hashes[relative_path] = file_hash\n\n    with open(metadata_path, 'w') as f:\n        json.dump(current_hashes, f)\n\n    for rel_path in set(previous_hashes) | set(current_hashes):\n        prev_hash = previous_hashes.get(rel_path)\n        curr_hash = current_hashes.get(rel_path)\n\n        # File has been removed\n        if not curr_hash and prev_hash:\n            os.remove(os.path.join(backup_dir, rel_path))\n        \n        # File is new or modified\n        elif curr_hash != prev_hash:\n            backup_file_path = os.path.join(backup_dir, rel_path)\n            os.makedirs(os.path.dirname(backup_file_path), exist_ok=True)\n            with open(filepath, 'rb') as src, open(backup_file_path, 'wb') as dest:\n                dest.write(src.read())\n\n# Example usage\nsource_directory = "/path/to/usb"\nbackup_directory = "/backup/location"\nmetadata_json = os.path.join(backup_directory, "metadata.json")\n\ncreate_backup(source_directory, backup_directory, metadata_json)