from bs4 import BeautifulSoup\n\ndef get_largest_text_block(html_content):\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    def find_max_text_element(element):\n        max_len = 0\n        max_element = None\n        \n        # Check if the element itself has significant text (not counting tags)\n        text = ''.join(element.find_all(text=True))\n        current_length = len(''.join(text.split()))\n        \n        # If this element has more text than all its children combined, consider it as a candidate\n        for child in element.children:\n            if not isinstance(child, str):  # skip over strings that are just text parts here\n                current_length -= find_max_text_element(child)[1]\n        \n        if current_length > max_len:\n            max_len = current_length\n            max_element = element\n        \n        # Recursively check children\n        for child in element.children:\n            child_element, child_length = find_max_text_element(child)\n            if child_length > max_len:\n                max_len = child_length\n                max_element = child_element\n        \n        return max_element, max_len\n    \n    body_content = soup.find('body')\n    largest_block, _ = find_max_text_element(body_content)\n    \n    # Return the text of the element with the most significant text content\n    return ''.join(largest_block.find_all(text=True))\n\n# Example HTML content\nhtml_data = """\n<html>\n   <body>\n      <div id="header">This is the header we don't care about</div>\n      <div id="content">This is the <b>Main Page</b> content.  it is the\n      longest block of text in this document and should be chosen as\n      most likely being the important page content.</div>\n   </body>\n</html>\n"""\n\nprint(get_largest_text_block(html_data))