def find_common_blocks(f1_path, f2_path, min_block_size=4):\n    BLOCK_SIZE = 4096  # Read block size\n    \n    def read_file_in_chunks(file_path):\n        with open(file_path, 'rb') as file:\n            while True:\n                data = file.read(BLOCK_SIZE)\n                if not data:\n                    break\n                yield data\n                \n    def find_common_hashes(data1, data2, min_block_size):\n        from collections import defaultdict\n        \n        hash_dict = defaultdict(list)\n        \n        # Hash each possible block in f1\n        for i in range(len(data1) - min_block_size + 1):\n            block = data1[i:i+min_block_size]\n            hash_dict[block].append((1, i))\n            \n        common_blocks = []\n        \n        # Compare blocks from f2 with hashed blocks from f1\n        for j in range(len(data2) - min_block_size + 1):\n            block = data2[j:j+min_block_size]\n            if block in hash_dict:\n                for (file_id, start_index) in hash_dict[block]:\n                    common_blocks.append((block, file_id, start_index, 2, j))\n        \n        return common_blocks\n\n    def print_common_blocks(common_blocks):\n        seen = set()\n        for block, f1_id, f1_start, f2_id, f2_start in sorted(common_blocks, key=lambda x: (x[0], -len(x[0]))):\n            if (block, f1_start, f2_start) not in seen:\n                print(f'length {len(block)}: "{block.decode(errors="ignore")}" in f{f1_id}@{f1_start} and f{f2_id}@{f2_start}')\n                seen.add((block, f1_start, f2_start))\n                    \n    data1 = b''.join(read_file_in_chunks(f1_path))\n    data2 = b''.join(read_file_in_chunks(f2_path))\n    \n    common_blocks = find_common_hashes(data1, data2, min_block_size)\n    print_common_blocks(common_blocks)\n\n# Example usage\nfind_common_blocks('f1.txt', 'f2.txt')