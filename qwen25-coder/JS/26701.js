const puppeteer = require('puppeteer');\n\n   (async () => {\n     const browser = await puppeteer.launch();\n     const page = await browser.newPage();\n     await page.goto('https://example.com/', {waitUntil: 'networkidle2'});\n     \n     // Capture all links on the main page\n     let urls = await page.$$eval("a[href^='/']", elements => \n         elements.map(e => e.href)\n     );\n\n     // Create the directory structure and download each page as static HTML\n     for (let url of urls) {\n       try {\n         await page.goto(url, {waitUntil: 'networkidle2'});\n         let htmlContent = await page.content();\n         const fs = require('fs');\n         const path = './static' + new URL(url).pathname;\n         fs.mkdirSync(path.substring(0, path.lastIndexOf('/')), { recursive: true });\n         fs.writeFileSync(path.endsWith('/') ? path + 'index.html': path+'.html', htmlContent);\n       } catch (e) {\n         console.warn(`Error capturing ${url}:`, e.message);\n       }\n     }\n\n     await browser.close();\n   })();