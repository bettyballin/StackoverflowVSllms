// Simple Vector Addition Example in CUDA\n\n#include <iostream>\n#include <cuda_runtime.h>\n\n__global__ void addKernel(int *c, const int *a, const int *b) {\n    int i = threadIdx.x;\n    c[i] = a[i] + b[i];\n}\n\nint main() {\n    const int arraySize = 512;\n    const int a[arraySize] = {0}; // initialize with zeros\n    const int b[arraySize] = {1}; // initialize with ones\n\n    // host copies of a, b, c\n    int *h_a, *h_b, *h_c;\n\n    // device copies of a, b, c\n    int *d_a, *d_b, *d_c;\n\n    size_t size = arraySize * sizeof(int);\n\n    // Allocate space for host copies of a, b, c\n    h_a = (int *)malloc(size);\n    h_b = (int *)malloc(size);\n    h_c = (int *)malloc(size);\n\n    // Allocate space for device copies of a, b, c\n    cudaMalloc((void **)&d_a, size);\n    cudaMalloc((void **)&d_b, size);\n    cudaMalloc((void **)&d_c, size);\n\n    // Setup input values\n    for (int i = 0; i < arraySize; i++) {\n        h_a[i] = a[i];\n        h_b[i] = b[i];\n    }\n\n    // Copy inputs to device\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    // Launch addKernel() on GPU with one block of arraySize threads\n    addKernel<<<1, arraySize>>>(d_c, d_a, d_b);\n\n    // Copy result back to host\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n\n    // Clean up host memory\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    return 0;\n}